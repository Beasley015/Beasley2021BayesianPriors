---
title: 'Appendix S2: Tutorial for creating aggregated priors using R and JAGS'
author: 'Emily M. Beasley'
subtitle: |
  | Ecologically informed priors improve Bayesian model estimates of species richness and occupancy for undetected species
  | *Ecological Applications*
output: pdf_document
urlcolor: blue
bibliography: Ch1InformedPriors.bib
csl: ecological-applications.csl
header-includes:
  - \renewcommand{\figurename}{Figure S}
  - \makeatletter
  - \def\fnum@figure{\figurename\thefigure}
  - \makeatother
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F)
```

This appendix is a tutorial for using prior aggregation to include external sources of information in multi-species occupancy models (MSOMs). Running the code included in this tutorial requires the software JAGS, which can be downloaded [here](https://sourceforge.net/projects/mcmc-jags/files/). 

This tutorial does not include general information on Bayesian MSOMs or the use and selection of ecologically informed priors. For an introduction to Bayesian MSOMs, see Chapter 11 of *Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in R and BUGS* by Royle and K$\'{e}$ry [-@kery_applied_2015]. For a guide to Bayesian model selection, see Hooten and Hobbs [-@hooten_guide_2015]. For a guide to Bayesian model checking, see Conn et al. [-@conn_guide_2018]. For more information on developing ecologically informed priors, see Low Choy et al. [-@choy_elicitation_2009], Banner et al. [-@banner_use_2020], and citations therein.

This tutorial requires the following packages:
  
```{r, message=F}
library(R2jags)
library(boot)
library(abind)
library(tidyverse)
library(ggnewscale)

# Set seed for reproducibility:
set.seed(23)
```

### 1. Simulate the community

We will begin by simulating a community consisting of 10 species. We will assume that we surveyed this community by sampling 20 sites over a period of 4 surveys each. We will also simulate a covariate that is correlated with the occupancy rates of some species: half of the species in the community will respond negatively to the covariate, while the other half are not affected.
  
```{r}
# Global variables
nspec <- 10 # number of species
nsite <- 20 # number of sites
nsurvey <- 4 # surveys per site

Ks <- rep(nsurvey, nsite) # vector of surveys at each site

# Vector of species-specific linear relationships between
# the environmental covariate & occupancy probability: 
# half of species respond negatively
resp2cov <- c(rnorm(n = 5, sd = 0.25),
              rnorm(n = 5, mean = -3, sd = 0.25))

resp2cov <- sample(resp2cov)

# Covariate values for sites
cov <- sort(rnorm(n = nsite))
```

To simulate site-level occupancy, we will first draw species-level occupancy probabilities from a beta distribution $\psi_i \sim Beta(\alpha = 2, \beta = 3)$. This distribution generates a wide range of occupancy probabilities (95% interval `r qbeta(p = c(0.025, 0.975), shape1 = 2, shape2 = 3)[1]` -- `r qbeta(p = c(0.025, 0.975), shape1 = 2, shape2 = 3)[2]`), a situation in which data augmentation is known to work well. We will use a logit link function to account for covariate effects on site-level occupancy probability of each species. Finally, the true occupancy state for each species at each site will be the result of a Bernoulli trial with the site-level probability as the probability of success.
  
```{r}
# Get probs from a beta distribution
sim.occ <- rbeta(n = nspec, shape1 = 2, shape2 = 3)

# Write function to simulate true occupancy state
tru.mats <- function(spec=nspec, site=nsite,
                     alpha1=resp2cov){
  
  #Get site-level psi to account for covariates
  alpha0 <- logit(sim.occ)
  
  #Create empty matrix to store occupancy probs
  logit.psi <- matrix(NA, nrow = spec, ncol = site)
  
  # Generate occupancy probs
  for(i in 1:spec){
    logit.psi[i,] <- alpha0[i] + alpha1[i]*cov
  }
  
  # Transform
  psi <- plogis(logit.psi)
  
  # Generate true occupancy state
  nlist<-list()
  for(a in 1:spec){
    nlist[[a]] <- rbinom(n = site, size = 1, prob = psi[a,])
  }
  
  #Turn abundance vectors into abundance matrix
  ns<-do.call(rbind, nlist)
  
  return(ns)
}

# Get true occupancy states
tru <- tru.mats()
```

Similarly to species-level occupancy probabilities, species-level detection probabilities will be drawn from a beta distribution $p_i \sim Beta(\alpha = 2, \beta = 8)$. This will generate low-to-mid detection probabilites (95% interval `r qbeta(p = c(0.025, 0.975), shape1 = 2, shape2 = 8)[1]` -- `r qbeta(p = c(0.025, 0.975), shape1 = 2, shape2 = 8)[2]`), another situation in which data augmentation performs well. Environmental or survey covariates that may influence detectability can be added using the logit link function; however, for this example we will assume detectability does not vary across sites and surveys.

Simulated survey data will be the result of a Bernoulli trial with the species-level detection probability as the probability of encountering that species at a given site during a given survey.
  
```{r}
# Generate mean detection probabilities from beta dist
mean.p <- rbeta(n = nspec, shape1 = 2, shape2 = 8)
mean.p <- sort(mean.p, decreasing = T)

# Generate detection histories
get.obs <- function(mat, specs){
  #Detection intercept and cov responses
  beta0<-logit(mean.p) #put it on logit scale

  #Logit link function
  logit.p <- array(NA, dim = c(nsite, nsurvey, specs))
  for(i in 1:specs){
    for(j in 1:nsite){
      for(k in 1:nsurvey){
        logit.p[j,,i] <- beta0[i] # Add covariates here
      }
    }
  }

  p <- plogis(logit.p)

  #Simulate observation data
  L<-list()

  for(b in 1:specs){
    y<-matrix(NA, ncol = nsite, nrow = nsurvey)
    for(a in 1:nsurvey){
      y[a,]<-rbinom(n = nsite, size = 1, prob = p[,,b]*mat[b,])
    }
    L[[b]]<-t(y)
  }

  #Smash it into array
  obs<-array(as.numeric(unlist(L)), 
                 dim=c(nsite, nsurvey, specs))

  return(obs)
}

obs.data <- get.obs(mat = tru, specs = nspec)

# Look at observed occurrence
maxobs <- apply(obs.data, c(1,3), max)
```

By calculating the column sums, we can see that one species went undetected in the simulated survey: 
  
```{r}
colSums(maxobs) # One species was not observed
```

To make the JAGS script easier to write and the figures more readable, the undetected species was moved to the last column in the observed data:
  
```{r, include = T}
# Remove undetected species
obs.data <- obs.data[,,-which(colSums(maxobs) == 0)]

# Function to reorder true values
reorder <- function(x){
  if (length(dim(x)) == 0){
    nondets <- which(colSums(maxobs) == 0)
    copy <- x[nondets]
    x <- x[-nondets]
    new <- c(x, copy)
    return(new)
    }
  else {
    nondets <- which(colSums(maxobs) == 0)
    copy <- x[nondets,]
    x <- x[-nondets,]
    new <- rbind(x, copy)
    return(new)
    }
}

# Reorder simulated values
sim.occ <- reorder(sim.occ)
mean.p <- reorder(mean.p)
resp2cov <- reorder(resp2cov)
tru <- reorder(tru)
maxobs <- reorder(maxobs)

# Augment observed with all-zero encounter history as a control
ems.array <- array(0, dim = c(nsite, nsurvey, 1))
obs.aug <- abind(obs.data, ems.array, along = 3)
```

### 2. Define the informed prior

Next, we will define the informed species-level prior distribution for the undetected species. Although the priors can be defined in the main model text, writing them separately allows you to more easily to adjust the variance, relative weights, etc. of different prior combinations. Running models with different priors is recommended as a test for prior sensitivity.

Most Bayesian MSOMs use normally-distributed priors, but other distributions can be used. Code for aggregating non-normal distributions can be found in de Carvalho et al. [-@de_carvalho_choosing_2015]. We will define the mean of informed species-level prior using the true value of the simulated covariate. We know the true value of the covariate is:
  
```{r}
# Get true covariate value
resp2cov[10]
```

We will round this value to -3 as the mean of the informed prior distribution. We will also assign a variance of 0.5 (standard deviation of approximately 0.7). This value is somewhat arbitrary, but in general large standard deviations (> 2) are not recommended, as they can yield bimodal posterior distributions [@northrup_comment_2018].
  
We will use the Markov chain Monte Carlo (MCMC) sampler JAGS to analyze the model. JAGS is compatible with most operating systems and the language is similar to R. The package R2jags will allow us to call JAGS directly from R. The full JAGS script and raw Markdown file used to create this appendix can be found [here](https://github.com/Beasley015/Beasley2021BayesianPriors).

### 3. Run model

Before running the model, we need to send some information to JAGS, including our data, the parameters we want JAGS to return, and the initial values for the Markov chains.
  
```{r}
# List of data to send to model
datalist <- list(J = nsite, K = Ks, obs = obs.aug, 
                 spec = nspec, cov = cov)

# Parameters to save after model is analyzed
parms <- c('N', 'a0', 'b0', 'a1', 'Z', 'a1.mean','tau.a1', 'pooled.mean',
           'pooled.var')

# Initial values for the Markov chains
init.values<-function(){
  maxobs <- apply(obs.aug, c(1,3), max)
  inits <- list(
    w = rep(1,nspec),
    a0 = rnorm(n = nspec),
    a1 = rnorm(n = nspec),
    b0 = rnorm(n = nspec),
    Z = maxobs)
}
```

Finally, run the model in JAGS. Additional code for saving and loading the model results can be found in the [Github repository](https://github.com/Beasley015/Beasley2021BayesianPriors).
  
```{r, echo = T, warning=F, results='hide', message = F}
# Send model to JAGS
model <- jags(model.file = 'samplemod.txt', data = datalist,
              n.chains = 3, parameters.to.save = parms,
              inits = init.values, n.burnin = 1000,
              n.iter = 5000, n.thin = 3)
```

```{r, include = F}
# Save/load model
# saveRDS(model, file = "sample_mod.rds")
# model <- readRDS(file = "sample_mod.rds")
```

### 4. Creating Figures

#### 4.1 Check to see if aggregation worked  
  
We can check if prior aggregation worked by comparing the posterior distribution (i.e. the model result) to the aggregated prior, and the aggregated prior to its parent distributions. If prior aggregation was successful, the aggregated prior should be somewhere in between the informed species-level prior and the community-level hyperprior. The posterior distribution should resemble the aggregated prior more than the two parent distributions.

We will start by extracting the mean and standard deviation of each prior from the model. Note that model parameters are either variance or precision (tau); these need to be converted to standard deviation.
  
```{r}
# Function to get value at peak of posterior distribution
get.peak <- function(param){
  # Get location of value that is highest on the y axis when 
  # plotting density function
  highest.y <- which.max(density(param)$y)
  
  # Find that value
  peak <- density(param)$x[highest.y]
  
  return(peak)
}

# Get values from aggregated prior
pooled.mean <- get.peak(model$BUGSoutput$sims.list$pooled.mean) 
pooled.sd <- get.peak(sqrt(model$BUGSoutput$sims.list$pooled.var))

# Create objects from informed values used in priors
inf.mean <- -3
inf.sd <- sqrt(1/0.5)
  
# Pull community distribution priors from model
comm.mean <- get.peak(model$BUGSoutput$sims.list$a1.mean)
comm.sd <- get.peak(sqrt(1/model$BUGSoutput$sims.list$tau.a1))
  
# Pull posteriors from model
post.mean <- mean(model$BUGSoutput$sims.list$a1[,10])
post.sd <- sd(model$BUGSoutput$sims.list$a1[,10])
```
  
We will compare the distributions using ggplot:  
  
```{r Figure S1, fig.cap="Comparison of the posterior distribution of the undetected species (red line) and the priors (black lines). The aggregated prior (solid black line) should fall somewhere in between the informed species-level prior (dotted black line) and uninformed community-level prior (dashed black line).", fig.width=5, fig.height=3}

# Plot the distributions
ggplot()+
  stat_function(fun = dnorm, n = 1000, 
                args = list(mean = pooled.mean, sd = pooled.sd),
                size = 1, aes(linetype = "Aggregated", color = "Prior"))+
  stat_function(fun = dnorm, n = 1000, 
                args = list(mean = inf.mean, sd = inf.sd),
                size = 1, aes(linetype = "Informed", color = "Prior"))+
  stat_function(fun = dnorm, n = 1000, 
                args = list(mean = comm.mean, sd = comm.sd),
                size = 1, aes(linetype = "Community", color = "Prior"))+
  stat_function(fun = dnorm, n = 1000, 
                args = list(mean = post.mean, sd = post.sd),
                size = 1, aes(linetype = "Aggregated", color = "Posterior"))+
  xlim(c(-6, 5))+
  scale_linetype_manual(breaks = c("Aggregated", "Informed", "Community"),
                        values = c(1, 3, 5), name = "Prior")+
  scale_color_manual(breaks = c("Prior", "Posterior"),
                     values = c("black", "red"), name = "")+
  labs(y = "Density")+
  theme_bw(base_size = 16)+
  theme(panel.grid = element_blank(), 
        axis.title.x = element_blank())
```

Based on this figure, prior aggregation was successful. The posterior distribution (red) is most similar to the aggregated prior (solid black line). Note that the posterior has been pulled slightly towards the center of the community-level prior (dashed black line): this is normal, and occurs as a result of modeling all species in the context of the community.  

#### 4.2 Regional richness estimates

Next, we will evaluate whether the model successfully accounted for the regional occurrence of the undetected species. First we will extract the posterior distribution of the parameter N, or regional species richness, from the model. To determine whether the model accounted for the missing species, you can use a measure of centrality such as the median:
  
```{r}
# Extract regional species richness N from model
Ns <- as.vector(model$BUGSoutput$sims.list$N)

# Create table of counts for each estimate
Ns %>%
  table() %>%
  data.frame() %>%
  {. ->> ns.frame}
colnames(ns.frame) <- c("N_Species", "Freq")
  
# Look at mean and median estimates
median(Ns)
```

Or, more commonly, the expected value for the parameter (i.e. the peak of the posterior probability distribution, Figure S2). For our simulated data, the expected value and median estimates agree on a regional richness estimate of 10 species.
  
```{r Figure S2, fig.cap="Posterior distribution of estimated regional species richness. The expected regional richness value, or the peak of the distribution, is 10 species, meaning the model successfully accounted for the undetected species.", fig.width=3, fig.height=3}
# Check it graphically
Ns.median <- median(Ns)
ggplot(data = ns.frame, aes(x = as.integer(as.character(N_Species)),
                            y = Freq))+
  geom_col(width = 0.95, color = 'lightgray')+
  scale_x_discrete(limits = c(9,10))+
  labs(x = "Estimated Richness (N)", y = "Frequency")+
  scale_y_continuous(expand = c(0,0))+
  theme_classic(base_size = 14)+
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank(),
        legend.key.height = unit(40, units = 'pt'),
        aspect.ratio = 1/1)
  
```

#### 4.3 Covariate responses

To examine individual species' responses to the environmental covariate, we begin by extracting the parameter from the JAGS object and adding labels denoting species IDs:
  
```{r}
# Extract covariate estimates from jags object
a1s <- model$BUGSoutput$sims.list$a1
  
a1s <- as.data.frame(a1s)
  
# Create a vector of species names
specnames <- logical()
for(i in 1:nspec){
  specnames[i] <- paste("Spec", i, sep = "")
}
  
colnames(a1s) <- specnames
```

Next, pivot the data from wide to long format for easier plotting, and calculate summary statistics. Usually, the best method for evaluating species' responses is by viewing the 95% credible interval (CI) and using the mean as the measure of centrality:
  
```{r, warning=FALSE, message = F}
# Pivot data frame for plotting
a1.long <- a1s %>%
  pivot_longer(cols = everything(), names_to = "Spec", 
               values_to = "a1")

a1.long$Spec <- factor(a1.long$Spec, levels = specnames)

# Get summary stats
a1.stat <- a1.long %>%
  group_by(Spec) %>%
  summarise(mean = mean(a1), lo = quantile(a1, 0.025), 
            hi = quantile(a1, 0.975)) %>%
  mutate(tru.resp = resp2cov)
```

Create the plot using ggplot:
  
```{r Figure S3, fig.cap = "Estimated species-level responses to the simulated covariate. Mean estimates are denoted by black dots, whereas the true, simulated values are denoted with red dots. Error bars represent the 95% credible interval (CI); a CI which does not overlap 0 is usually considered significant.", fig.width=6, fig.height=3}
# Make interval plot
ggplot(data = a1.stat, aes(x = Spec, y = mean))+
  geom_point(size = 1.5)+
  geom_errorbar(ymin = a1.stat$lo, ymax = a1.stat$hi, 
                size = 1, width = 0.5)+
  geom_point(aes(y = tru.resp), color = "red", size = 1.5)+
  geom_hline(yintercept = 0, linetype = "dashed", size = 1)+
  scale_y_continuous(limits = c(-25, 20))+
  labs(x = "Species", y = "Coefficient")+
  theme_bw(base_size = 14)+
  theme(panel.grid = element_blank())
```

The model correctly estimated significant negative covariate responses for detected species 1--4 and the undetected species 10 (Figure S3). Based on the position of the mean (black dots) relative to the 95% CI, we can also deduce that the posterior distributions for the detected species are highly skewed, with long tails extending away from zero. By contrast, the "stabilizing" effect of informed priors is clear in the model estimate for species 10, which has a more symmetrical and precise posterior distribution.

### 5. Literature Cited